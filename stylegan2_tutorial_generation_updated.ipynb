{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylegan2_tutorial_generation_updated.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrober24/SL_2021-2022/blob/main/stylegan2_tutorial_generation_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUXWCtaqslv"
      },
      "source": [
        "# Stylegan2: Generating images\n",
        "This will install all the necessary libraries to use the StyleGAN2 repo. Press the play button or `shift+return` to run each cell.\n",
        "\n",
        "Only run the next cell once per session.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2FQJl-S8ZGg"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QtGFQ_PwLFZ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cfa158-8176-49f3-aa92-b2751f185aad"
      },
      "source": [
        "#@title Set tensorflow version\n",
        "#@markdown always use tensorflow1\n",
        "\n",
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl_HcPhkTsv",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5769631d-5498-4cda-b70f-cde0e95c2ae9"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "zhu9pBu7j4Rs"
      },
      "source": [
        "#@title Install StyleGAN2 Repository\n",
        "#@markdown StyleGAN2 will be installed to your Google Drive to speed up the training process\n",
        "\n",
        "#@markdown Run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary.\n",
        "import os\n",
        "import shlex\n",
        "!pip install opensimplex\n",
        "if os.path.isdir(\"/content/drive/MyDrive/stylegan2-aug-colab\"):\n",
        "  %cd \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2\"\n",
        "elif os.path.isdir(\"/content/drive\"):\n",
        "  %cd /content/drive/MyDrive/\n",
        "  !mkdir stylegan2-aug-colab\n",
        "  %cd stylegan2-aug-colab/\n",
        "  !git clone -b augs-attn https://github.com/dvschultz/stylegan2\n",
        "  %cd stylegan2\n",
        "  !mkdir raw_images\n",
        "  !mkdir pkl\n",
        "  %cd pkl\n",
        "  !gdown --id 1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF #inception: https://drive.google.com/open?id=1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF\n",
        "  %cd ../\n",
        "  !mkdir results\n",
        "  !mkdir results/00001-pretrained\n",
        "  %cd results/00001-pretrained\n",
        "  !gdown --id 1UlDmJVLLnBD9SnLSMXeiZRO6g-OMQCA_\n",
        "  %cd ../../\n",
        "  %mkdir datasets\n",
        "else:\n",
        "  !git clone -b augs-attn https://github.com/dvschultz/stylegan2\n",
        "  %cd styelgan2\n",
        "  !mkdir raw_images\n",
        "  !mkdir pkl\n",
        "  %cd pkl\n",
        "  !gdown --id 1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF #inception: https://drive.google.com/open?id=1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF\n",
        "  %cd ../\n",
        "  !mkdir results\n",
        "  !mkdir results/00001-pretrained\n",
        "  %cd results/00001-pretrained\n",
        "  !gdown --id 1UlDmJVLLnBD9SnLSMXeiZRO6g-OMQCA_\n",
        "  %cd ../../\n",
        "  %mkdir datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Network Selection\n",
        "#@markdown This is where you will choose the model you would like to generate images from.\n",
        "\n",
        "#@markdown change the file path here to the pkl of your most recent training session found in \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/results/{model-name}\" \n",
        "network = \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/results/00057-stylegan2-tech-1gpu-config-f/network-snapshot-010024.pkl\" #@param {type: \"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vo8VkrEalL1f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9vCDt9LRtXl"
      },
      "source": [
        "# Seed Generation\n",
        "\n",
        "##Options\n",
        "\n",
        "`--seeds`\n",
        "\n",
        "This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
        "\n",
        "\n",
        "\n",
        "*   For single image, choose a single seed number: 23\n",
        "*   For a range of seeds: `[first]`-`[last]` (1-10) \n",
        "*   For List of seeds: `seed1`,`seed2`,`seed3`,...`seedn` (1,12,34,125)\n",
        "  * no spaces in any lists or ranges\n",
        "\n",
        "\n",
        "\n",
        "`--truncation-psi`\n",
        "\n",
        "Truncation is a special argument of StyleGAN. Essentially values that are closer to 0 will be more real than numbers further away from 0. I generally recommend a value between `0.5` and `1.0`. `0.5` will give you pretty \"realistic\" results, while `1.0` is likely to give you \"weirder\" results. Truncation is looked at more closely in the next section.\n",
        "\n",
        "note: run !python run_generator.py -h  for a list of all possible options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3MhXEAMOMXH",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "seeds = \"5\" #@param {type: \"string\"}\n",
        "truncation_psi = 0.7 #@param {type: \"number\"}\n",
        "!python run_generator.py generate-images --network=$network --seeds=$seeds --truncation-psi=$truncation_psi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMiqkA3IReZB"
      },
      "source": [
        "##zip the generated files and download them.\n",
        "\n",
        "`zip_input`: path to the folder you would like to zip\n",
        "`zip_output`: path to folder where the zip file will be **placed**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **zip files**\n",
        "zip_input = \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/results/00090-generate-images/\" #@param\n",
        "zip_output = \"/content/example.zip\" #@param\n",
        "!zip -r $zip_output $zip_input"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x71PLCUXsgmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Download Zip**\n",
        "from google.colab import files\n",
        "files.download(zip_output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cgBE0Nm2tV0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqUBp73Yfsua"
      },
      "source": [
        "#Near neighbors\n",
        "**Added by popular demand**\n",
        "\n",
        "Let’s say you have a seed you like, but want to see other images like it to see if there’s something better. Now you can with the `near-neighbor` argument.\n",
        "\n",
        "### Options\n",
        "`--network`, `--seeds`, and `--truncation_psi` work the same as above.\n",
        "\n",
        "`--diameter`: this sets how far away from the seed you want to generate images. `.0000001` is really close, `.5` is reallly far.\n",
        "\n",
        "`--num_samples`: how many samples you want to produce\n",
        "\n",
        "`--save_vector`: this will save the vector as a file in the .npy format. You can then use this for interpolation (not super well supported right now, but can be used manually—see the Projection code for an example of reading a .npy file and interpolating it).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zadz8O95hCxf",
        "cellView": "form"
      },
      "source": [
        "#@title Generate Near Neighbors\n",
        "seeds = \"5\" #@param {type:\"string\"}\n",
        "truncation_psi = 0.5 #@param {type:\"number\"}\n",
        "num_samples = 2 #@param {type: \"number\"}\n",
        "diameter = 0.5 #@param {type: \"number\"}\n",
        "!python run_generator.py generate-neighbors --network=$network --seeds=$seeds --truncation-psi=$truncation_psi --num_samples=$num_samples --diameter=$diameter --save_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HFUHHVgB6Xt"
      },
      "source": [
        "# Linear Interpolation\n",
        "\n",
        "Linear interpolation will generate a linear path from one seed to another. The makers of StyleGAN say that doing this in the w space produces the best disentangled interpolations. \n",
        "\n",
        "`walk_type` : can be line-z or line-w. Line-z is the standard; however, z-space is entangled so the output may not appear completely linear. w-space is disentangled and thus will produce more linear outputs. It is recommended to start with line-z\n",
        "\n",
        "`--seeds`: Use images you generated to control the interpolation points. If your first and last seed are the same this will produce a loop (nice for Instagram and gifs!)\n",
        "\n",
        "`frames`: The total number of frames to be generated. Seeds will be spread out evenly in frame range\n",
        "\n",
        "`truncation_psi`: same as previous steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ2rYIC4TdaJ",
        "cellView": "form"
      },
      "source": [
        "#@title Generate Linear Interpolation\n",
        "walk_type = 'line-w' #@param ['line-z','line-w']\n",
        "seeds = \"1,20\" #@param {type: \"string\"}\n",
        "frames = 3 #@param {type: \"number\"}\n",
        "trucation_psi = 0.7 #@param {type: \"number\"}\n",
        "\n",
        "!python run_generator.py generate-latent-walk --network=$network --walk-type=$walk_type --seeds=$seeds --frames $frames --truncation-psi=$truncation_psi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Generate a video from the frames\n",
        "\n",
        " `foldernum`: number of the folder created by the interpolation script. (Example: for folder \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/results/00097-generate-latent-walk\" `foldernum`=00097)\n",
        "\n",
        "`framerate`: framerate of video. Can be any number you'd like (24 is standard cinematic framerate)\n",
        "\n",
        "`video_name`: name of the video to be created\n",
        "\n",
        "videos can be found at \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/{yourvideo.mp4}\""
      ],
      "metadata": {
        "id": "Y6NzZfmi8fsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Produce Video \n",
        "foldernum = \"00097\" #@param {type: \"string\"}\n",
        "folderpath = \"./results/\" + foldernum + \"-generate-latent-walk/frame%05d.png\"\n",
        "framerate = 24 #@param {type: \"number\"}\n",
        "video_name = \"interpolation.mp4\" #@param {type: \"string\"}\n",
        "\n",
        "!ffmpeg -r $framerate -i $folderpath -vcodec libx264 -pix_fmt yuv420p $video_name"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CYGbOnAI6pab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg14LAr05NF1"
      },
      "source": [
        "# Truncation\n",
        "Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. Most people choose between 0.5 and 1.0, but technically its infinite. \n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "###Options \n",
        "\n",
        "`--seed`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSB_NQ_J5wYa",
        "cellView": "form"
      },
      "source": [
        "#@title Generate Truncation Frames\n",
        "seed = 1 #@param {type:\"number\"}\n",
        "start = -1.0 #@param {type: \"number\"}\n",
        "stop = 1.0 #@param {type: \"number\"}\n",
        "increment = 0.05 #@param {type: \"number\"}\n",
        "\n",
        "!python run_generator.py truncation-traversal --network=$network --seed=$seed --start=$start --stop=$stop --increment=$increment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Produce Truncation Video \n",
        "foldernum = \"00097\" #@param {type: \"string\"}\n",
        "folderpath = \"./results/\" + foldernum + \"-truncation-traversal/frame%05d.png\"\n",
        "framerate = 24 #@param {type: \"number\"}\n",
        "video_name = \"truncation.mp4\" #@param {type: \"string\"}\n",
        "\n",
        "!ffmpeg -r $framerate -i $folderpath -vcodec libx264 -pix_fmt yuv420p $video_name"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fTSOjMiyCbzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDFOMs8lJEOL"
      },
      "source": [
        "#Projection\n",
        "Projection is the process of taking an image from outside the model and finding it’s nearest representation inside the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvD373CQKGNe",
        "cellView": "form"
      },
      "source": [
        "#@title Projection Functions\n",
        "#@markdown This sets up all of the functions we need for projection\n",
        "\n",
        "!gdown --id 1gbxwfHNOaGjGsLNTmmSrNA85X2VWHHOq -O /content/drive/MyDrive/stylegan2-aug-colab/stylegan2/pkl/vgg16_zhang_perceptual.pkl\n",
        "\n",
        "network_pkl = network\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "\n",
        "import pretrained_networks\n",
        "\n",
        "# Convert uploaded images to TFRecords\n",
        "import dataset_tool\n",
        "\n",
        "# Run the projector\n",
        "import training.dataset\n",
        "import training.misc\n",
        "import projector\n",
        "import os \n",
        "\n",
        "# Taken from https://github.com/alexanderkuk/log-progress\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n",
        "\n",
        "def interpolate(zs, steps,type='linear'):\n",
        "  out = []\n",
        "  for i in range(len(zs)-1):\n",
        "    c = zs[i+1]-zs[i]\n",
        "\n",
        "    for index in range(steps):\n",
        "      fraction = index/float(steps) # t/d\n",
        "      \n",
        "      # translated from: https://github.com/danro/jquery-easing/blob/master/jquery.easing.js\n",
        "      # see https://easings.net/ for examples\n",
        "      if type == 'linear':\n",
        "        out.append( c * fraction + zs[i] ) # c*(t/d)+b\n",
        "      elif type == 'easeInSine':\n",
        "        out.append( -c * np.cos(fraction * (np.pi/2)) + c + zs[i] ) # -c * Math.cos(t/d * (Math.PI/2)) + c + b\n",
        "      elif type == 'easeOutSine':\n",
        "        out.append( c * np.sin(fraction * (np.pi/2)) + zs[i]) # c * Math.sin(t/d * (Math.PI/2)) + b\n",
        "      elif type == 'easeInOutSine':\n",
        "        out.append(-c/2 * (np.cos(np.pi*fraction) - 1.0) + zs[i]) # -c/2 * (Math.cos(Math.PI*t/d) - 1) + b;\n",
        "      elif type == 'easeInQuad':\n",
        "        out.append(c * fraction * fraction + zs[i]) # c*(t/=d)*t + b;\n",
        "      elif type == 'easeOutQuad':\n",
        "       out.append(-c * fraction * (fraction-2) + zs[i]) # -c *(t/=d)*(t-2) + b;\n",
        "      # elif type == 'easeInOutQuad':\n",
        "      #   if(fraction/2 < 1):\n",
        "      #     out.append( ((c/2)*fraction*fraction) + zs[i]) #if ((t/=d/2) < 1) return c/2*t*t + b;\n",
        "      #   else:\n",
        "\t\t  #     out.append( (-c/2) * ((index-=1)*(index-2) - 1) + zs[i]; #return -c/2 * ((--t)*(t-2) - 1) + b;\n",
        "      else: \n",
        "        out.append( c * fraction + zs[i] ) # c*(t/d)+b\n",
        "  return out\n",
        "\n",
        "def saveImgs(imgs, location):\n",
        "  for idx, img in log_progress(enumerate(imgs), size = len(imgs), name=\"Saving images\"):\n",
        "    file = location + ('%05d.png' % (idx))\n",
        "    img.save(file)\n",
        "\n",
        "def generate_images_in_w_space(dlatents, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    Gs_kwargs.truncation_psi = truncation_psi\n",
        "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "\n",
        "    imgs = []\n",
        "    for row, dlatent in enumerate(dlatents):\n",
        "        #row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
        "        dl = (dlatent-dlatent_avg)*truncation_psi   + dlatent_avg\n",
        "        row_images = Gs.components.synthesis.run(dlatent,  **Gs_kwargs)\n",
        "        imgs.append(PIL.Image.fromarray(row_images[0], 'RGB'))\n",
        "    return imgs  \n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4GrJfy5KPEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "1f6f6c85-b73b-40ca-e79a-9e2aa8bb32ce"
      },
      "source": [
        "#@title make projection directories\n",
        "#@markdown Next, we need to make some folders so we can upload the image for projection. Only run this cell once per session.\n",
        "\n",
        "!mkdir projection\n",
        "!mkdir projection/imgs\n",
        "!mkdir projection/out\n",
        "!mkdir projection/records\n",
        "!mkdir projection/latents"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘projection’: File exists\n",
            "mkdir: cannot create directory ‘projection/imgs’: File exists\n",
            "mkdir: cannot create directory ‘projection/out’: File exists\n",
            "mkdir: cannot create directory ‘projection/records’: File exists\n",
            "mkdir: cannot create directory ‘projection/latents’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Projection Image Upload\n",
        "\n",
        "#@markdown Now upload a single image to stylegan2/projection/imgs (can also use the Files side panel). Image should be color PNG, with the same dimensions as your dataset.\n",
        "\n",
        "#@markdown run this cell twice to upload two images\n",
        "#change into imgs folder\n",
        "%cd ./projection/imgs\n",
        "\n",
        "#user input file uploader\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "#change back to stylegan2 directory\n",
        "%cd /content/drive/MyDrive/stylegan2-aug-colab/stylegan2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pKRKtEybH0dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiCR5knmMcQ1",
        "cellView": "form"
      },
      "source": [
        "#@title convert images into tfrecords\n",
        "\n",
        "#@markdown This will convert the images you just uploaded into tfrecords for projecting into the model\n",
        "\n",
        "\n",
        "#@markdown do not change any filepaths in this script\n",
        "!rm ./projection/records/*.*\n",
        "!python dataset_tool.py create_from_images_raw /content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/records/ProjectFiles /content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/imgs --result-dir= /content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nDUrNJjNp0U"
      },
      "source": [
        "### Project Options\n",
        "\n",
        "\n",
        "\n",
        "*  `num_images`: the number of images you want to project (same as number of images in the ./projection/imgs/ folder)\n",
        "*   `num_snapshots`: number of intermediate steps to save (max: 1000)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4fd0sT6N0pX",
        "cellView": "form"
      },
      "source": [
        "#@title Project Images\n",
        "!rm ./projection/*.*\n",
        "\n",
        "num_imaages = 2 #@param {type: \"number\"}\n",
        "num_snapshots = 5 #@param {type: \"number\"}\n",
        "!python run_projector.py project-real-images --network=$network --dataset=ProjectFiles --data-dir=/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/records --num-images=$num_images --num-snapshots=$num_snapshots --result-dir=/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2jtqgSjP12s"
      },
      "source": [
        "### Make an interpolation!\n",
        "\n",
        "This script will make a video interpolating between the two projected latent vectors from the last step.\n",
        "\n",
        "`latent_path1,2`: file path to .npy files created in previous step. (found in ./projection/latents \n",
        "\n",
        "`frame_count`: total number of frames to create\n",
        "\n",
        "`framerate`: same as previous steps\n",
        "\n",
        "`video_name`: name of the output interpolation video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6JyDtFsPKL3",
        "cellView": "form"
      },
      "source": [
        "\n",
        "\n",
        "latent_path1 = \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/latent0.npy\" #@param {type: \"string\"}\n",
        "latent_path2 = \"/content/drive/MyDrive/stylegan2-aug-colab/stylegan2/projection/latent1.npy\" #@param {type: \"string\"}\n",
        "latent1 = np.load(latent_path1)\n",
        "latent2 = np.load(latent_path2)\n",
        "frame_count = 144 #@param {type: \"number\"}\n",
        "\n",
        "imgs = generate_images_in_w_space(interpolate([latent1,latent2],frame_count,'linear'),0.7)\n",
        "\n",
        "!rm -rf interpolations\n",
        "%mkdir interpolations\n",
        "#save interpolation frames\n",
        "saveImgs(imgs,'./interpolations/')\n",
        "\n",
        "#make video\n",
        "framerate = 24 #@param {type: \"number\"}\n",
        "video_name = \"projected_interpolation.mp4\" #@param {type: \"string\"}\n",
        "!ffmpeg -r $framerate -i ./interpolations/%05d.png -vcodec libx264 -pix_fmt yuv420p $video_name\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}