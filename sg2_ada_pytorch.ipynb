{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sg2-ada-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrober24/SL1_2021/blob/main/sg2_ada_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZkTQZc7cMWN"
      },
      "source": [
        "# Custom Training StyleGan2-ADA\n",
        "\n",
        "In this notebook we will do transfer learning with StyleGAN2 and custom datasets.\n",
        "\n",
        "This means we will not train GAN on our images from scratch (as it takes about two weeks) but we will use the model already trained on the other images as a starting point. It will reduce training time to about 10 hours by skipping first stages where neural network learns low level features of images that are almost the same for any kind of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOgc3AOiY6iU",
        "outputId": "4ec2ef6c-eb34-4c51-f3da-fb4d8402203f"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "#@markdown Mount Google Drive to load pretrained models and to save the results.\n",
        "\n",
        "#@markdown After running this cell you will get the link. Follow the link, grant access to your Drive and copy auth code.\n",
        "\n",
        "#@markdown Paste the code to the input below and press Enter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6nP8w7IZDpb",
        "outputId": "058bac01-a85e-40b4-ee6e-f7015cb6f7de"
      },
      "source": [
        "#@title Install\n",
        "#@markdown StyleGAN2-ada will be installed to your Google Drive to speed up the training process\n",
        "\n",
        "#@markdown Run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary.\n",
        "import os\n",
        "import shlex\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-ada-pytorch\n",
        "    %cd colab-sg2-ada-pytorch\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    %cd pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
        "    %cd ../\n",
        "\n",
        "!pip install ninja opensimplex\n",
        "\n",
        "%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git pull\n",
        "!git stash\n",
        "!git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/colab-sg2-ada-pytorch\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 524, done.\u001b[K\n",
            "remote: Total 524 (delta 0), reused 0 (delta 0), pack-reused 524\u001b[K\n",
            "Receiving objects: 100% (524/524), 8.40 MiB | 11.25 MiB/s, done.\n",
            "Resolving deltas: 100% (298/298), done.\n",
            "/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
            "To: /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
            "382MB [00:14, 39.5MB/s]\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opensimplex, ninja\n",
            "Successfully installed ninja-1.10.2 opensimplex-0.3\n",
            "/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "Already up to date.\n",
            "Saved working directory and index state WIP on main: 464100c lil cleanup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MgOcCseZqlA",
        "outputId": "f64ab5a1-6670-40f8-ee36-6e41f7ce4cb8"
      },
      "source": [
        "#@title Data Preparation\n",
        "#@markdown Input image directory\n",
        "input_dir = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/raw_imgs/1024' #@param {type: \"string\"}\n",
        "#@markdown Path to the zip file where converted dataset will be stored\n",
        "dataset_file = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/house.zip' #@param {type: \"string\"}\n",
        "\n",
        "if not dataset_file.endswith('.zip'):\n",
        "  dataset_file += '.zip'\n",
        "input_dir = shlex.quote(input_dir)\n",
        "dataset_file = shlex.quote(dataset_file)\n",
        "!python dataset_tool.py --source {input_dir} --dest {dataset_file}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 400/400 [05:44<00:00,  1.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "E25JTmDbZX1z",
        "cellView": "form",
        "outputId": "3af5a995-c54b-49bf-a55b-442405a8b36a"
      },
      "source": [
        "#@title Train a custom model\n",
        "\n",
        "#@markdown Path to the dataset zip file\n",
        "dataset = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/house.zip\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown For transfer learning set it to `ffhq256`, `ffhq512` or `ffhq1024`accordingly to your images resolution.<br />\n",
        "#@markdown If you want to resume training process, provide the path to your latest .pkl file\n",
        "resume_from = \"ffhq1024\" #@param {type: \"string\"}\n",
        "\n",
        "dataset = shlex.quote(dataset)\n",
        "resume_from = shlex.quote(resume_from)\n",
        "#don't edit this unless you know what you're doing :)\n",
        "!python train.py --outdir ./results --snap=1 --cfg='11gb-gpu' --data={dataset} --aug=noaug --mirror=False --mirrory=False --metrics=None --resume={resume_from}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/house.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 400,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 1024\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 32768,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 10\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 4,\n",
            "  \"batch_gpu\": 4,\n",
            "  \"ema_kimg\": 10,\n",
            "  \"ema_rampup\": null,\n",
            "  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"./results/00000-house-11gb-gpu-noaug-resumeffhq1024\"\n",
            "}\n",
            "\n",
            "Output directory:   ./results/00000-house-11gb-gpu-noaug-resumeffhq1024\n",
            "Training data:      /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/house.zip\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   400\n",
            "Image resolution:   1024\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "Num images:  400\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "starting G epochs:  0.0\n",
            "Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\"\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl ... done\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator              Parameters  Buffers  Output shape         Datatype\n",
            "---                    ---         ---      ---                  ---     \n",
            "mapping.fc0            262656      -        [4, 512]             float32 \n",
            "mapping.fc1            262656      -        [4, 512]             float32 \n",
            "mapping.fc2            262656      -        [4, 512]             float32 \n",
            "mapping.fc3            262656      -        [4, 512]             float32 \n",
            "mapping.fc4            262656      -        [4, 512]             float32 \n",
            "mapping.fc5            262656      -        [4, 512]             float32 \n",
            "mapping.fc6            262656      -        [4, 512]             float32 \n",
            "mapping.fc7            262656      -        [4, 512]             float32 \n",
            "mapping                -           512      [4, 18, 512]         float32 \n",
            "synthesis.b4.conv1     2622465     32       [4, 512, 4, 4]       float32 \n",
            "synthesis.b4.torgb     264195      -        [4, 3, 4, 4]         float32 \n",
            "synthesis.b4:0         8192        16       [4, 512, 4, 4]       float32 \n",
            "synthesis.b4:1         -           -        [4, 512, 4, 4]       float32 \n",
            "synthesis.b8.conv0     2622465     80       [4, 512, 8, 8]       float32 \n",
            "synthesis.b8.conv1     2622465     80       [4, 512, 8, 8]       float32 \n",
            "synthesis.b8.torgb     264195      -        [4, 3, 8, 8]         float32 \n",
            "synthesis.b8:0         -           16       [4, 512, 8, 8]       float32 \n",
            "synthesis.b8:1         -           -        [4, 512, 8, 8]       float32 \n",
            "synthesis.b16.conv0    2622465     272      [4, 512, 16, 16]     float32 \n",
            "synthesis.b16.conv1    2622465     272      [4, 512, 16, 16]     float32 \n",
            "synthesis.b16.torgb    264195      -        [4, 3, 16, 16]       float32 \n",
            "synthesis.b16:0        -           16       [4, 512, 16, 16]     float32 \n",
            "synthesis.b16:1        -           -        [4, 512, 16, 16]     float32 \n",
            "synthesis.b32.conv0    2622465     1040     [4, 512, 32, 32]     float32 \n",
            "synthesis.b32.conv1    2622465     1040     [4, 512, 32, 32]     float32 \n",
            "synthesis.b32.torgb    264195      -        [4, 3, 32, 32]       float32 \n",
            "synthesis.b32:0        -           16       [4, 512, 32, 32]     float32 \n",
            "synthesis.b32:1        -           -        [4, 512, 32, 32]     float32 \n",
            "synthesis.b64.conv0    2622465     4112     [4, 512, 64, 64]     float32 \n",
            "synthesis.b64.conv1    2622465     4112     [4, 512, 64, 64]     float32 \n",
            "synthesis.b64.torgb    264195      -        [4, 3, 64, 64]       float32 \n",
            "synthesis.b64:0        -           16       [4, 512, 64, 64]     float32 \n",
            "synthesis.b64:1        -           -        [4, 512, 64, 64]     float32 \n",
            "synthesis.b128.conv0   1442561     16400    [4, 256, 128, 128]   float16 \n",
            "synthesis.b128.conv1   721409      16400    [4, 256, 128, 128]   float16 \n",
            "synthesis.b128.torgb   132099      -        [4, 3, 128, 128]     float16 \n",
            "synthesis.b128:0       -           16       [4, 256, 128, 128]   float16 \n",
            "synthesis.b128:1       -           -        [4, 256, 128, 128]   float32 \n",
            "synthesis.b256.conv0   426369      65552    [4, 128, 256, 256]   float16 \n",
            "synthesis.b256.conv1   213249      65552    [4, 128, 256, 256]   float16 \n",
            "synthesis.b256.torgb   66051       -        [4, 3, 256, 256]     float16 \n",
            "synthesis.b256:0       -           16       [4, 128, 256, 256]   float16 \n",
            "synthesis.b256:1       -           -        [4, 128, 256, 256]   float32 \n",
            "synthesis.b512.conv0   139457      262160   [4, 64, 512, 512]    float16 \n",
            "synthesis.b512.conv1   69761       262160   [4, 64, 512, 512]    float16 \n",
            "synthesis.b512.torgb   33027       -        [4, 3, 512, 512]     float16 \n",
            "synthesis.b512:0       -           16       [4, 64, 512, 512]    float16 \n",
            "synthesis.b512:1       -           -        [4, 64, 512, 512]    float32 \n",
            "synthesis.b1024.conv0  51297       1048592  [4, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.conv1  25665       1048592  [4, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.torgb  16515       -        [4, 3, 1024, 1024]   float16 \n",
            "synthesis.b1024:0      -           16       [4, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024:1      -           -        [4, 32, 1024, 1024]  float32 \n",
            "---                    ---         ---      ---                  ---     \n",
            "Total                  30370060    2797104  -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b1024.fromrgb  128         16       [4, 32, 1024, 1024]  float16 \n",
            "b1024.skip     2048        16       [4, 64, 512, 512]    float16 \n",
            "b1024.conv0    9248        16       [4, 32, 1024, 1024]  float16 \n",
            "b1024.conv1    18496       16       [4, 64, 512, 512]    float16 \n",
            "b1024          -           16       [4, 64, 512, 512]    float16 \n",
            "b512.skip      8192        16       [4, 128, 256, 256]   float16 \n",
            "b512.conv0     36928       16       [4, 64, 512, 512]    float16 \n",
            "b512.conv1     73856       16       [4, 128, 256, 256]   float16 \n",
            "b512           -           16       [4, 128, 256, 256]   float16 \n",
            "b256.skip      32768       16       [4, 256, 128, 128]   float16 \n",
            "b256.conv0     147584      16       [4, 128, 256, 256]   float16 \n",
            "b256.conv1     295168      16       [4, 256, 128, 128]   float16 \n",
            "b256           -           16       [4, 256, 128, 128]   float16 \n",
            "b128.skip      131072      16       [4, 512, 64, 64]     float16 \n",
            "b128.conv0     590080      16       [4, 256, 128, 128]   float16 \n",
            "b128.conv1     1180160     16       [4, 512, 64, 64]     float16 \n",
            "b128           -           16       [4, 512, 64, 64]     float16 \n",
            "b64.skip       262144      16       [4, 512, 32, 32]     float32 \n",
            "b64.conv0      2359808     16       [4, 512, 64, 64]     float32 \n",
            "b64.conv1      2359808     16       [4, 512, 32, 32]     float32 \n",
            "b64            -           16       [4, 512, 32, 32]     float32 \n",
            "b32.skip       262144      16       [4, 512, 16, 16]     float32 \n",
            "b32.conv0      2359808     16       [4, 512, 32, 32]     float32 \n",
            "b32.conv1      2359808     16       [4, 512, 16, 16]     float32 \n",
            "b32            -           16       [4, 512, 16, 16]     float32 \n",
            "b16.skip       262144      16       [4, 512, 8, 8]       float32 \n",
            "b16.conv0      2359808     16       [4, 512, 16, 16]     float32 \n",
            "b16.conv1      2359808     16       [4, 512, 8, 8]       float32 \n",
            "b16            -           16       [4, 512, 8, 8]       float32 \n",
            "b8.skip        262144      16       [4, 512, 4, 4]       float32 \n",
            "b8.conv0       2359808     16       [4, 512, 8, 8]       float32 \n",
            "b8.conv1       2359808     16       [4, 512, 4, 4]       float32 \n",
            "b8             -           16       [4, 512, 4, 4]       float32 \n",
            "b4.mbstd       -           -        [4, 513, 4, 4]       float32 \n",
            "b4.conv        2364416     16       [4, 512, 4, 4]       float32 \n",
            "b4.fc          4194816     -        [4, 512]             float32 \n",
            "b4.out         513         -        [4, 1]               float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          29012513    544      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 42s       sec/tick 15.3    sec/kimg 3816.32 maintenance 86.9   cpumem 5.54   gpumem 11.35  augment 0.000\n",
            "tick 1     kimg 4.0      time 37m 19s      sec/tick 2132.4  sec/kimg 533.10  maintenance 4.5    cpumem 5.00   gpumem 7.05   augment 0.000\n",
            "tick 2     kimg 8.0      time 1h 13m 14s   sec/tick 2149.4  sec/kimg 537.34  maintenance 5.2    cpumem 5.90   gpumem 7.05   augment 0.000\n",
            "tick 3     kimg 12.0     time 1h 49m 05s   sec/tick 2146.1  sec/kimg 536.53  maintenance 4.8    cpumem 6.07   gpumem 7.05   augment 0.000\n",
            "tick 4     kimg 16.0     time 2h 24m 54s   sec/tick 2144.6  sec/kimg 536.15  maintenance 4.7    cpumem 5.58   gpumem 7.05   augment 0.000\n",
            "tick 5     kimg 20.0     time 3h 00m 43s   sec/tick 2144.2  sec/kimg 536.05  maintenance 5.0    cpumem 5.86   gpumem 7.05   augment 0.000\n",
            "tick 6     kimg 24.0     time 3h 36m 36s   sec/tick 2147.7  sec/kimg 536.93  maintenance 5.3    cpumem 5.86   gpumem 7.05   augment 0.000\n",
            "tick 7     kimg 28.0     time 4h 12m 28s   sec/tick 2146.1  sec/kimg 536.51  maintenance 5.2    cpumem 5.86   gpumem 7.05   augment 0.000\n",
            "tick 8     kimg 32.0     time 4h 48m 17s   sec/tick 2143.9  sec/kimg 535.97  maintenance 5.2    cpumem 5.77   gpumem 7.05   augment 0.000\n",
            "tick 9     kimg 36.0     time 5h 24m 03s   sec/tick 2141.1  sec/kimg 535.28  maintenance 4.9    cpumem 5.82   gpumem 7.05   augment 0.000\n",
            "tick 10    kimg 40.0     time 5h 59m 52s   sec/tick 2143.8  sec/kimg 535.96  maintenance 5.2    cpumem 5.94   gpumem 7.05   augment 0.000\n",
            "tick 11    kimg 44.0     time 6h 35m 38s   sec/tick 2141.0  sec/kimg 535.26  maintenance 5.2    cpumem 5.49   gpumem 7.05   augment 0.000\n",
            "tick 12    kimg 48.0     time 7h 11m 25s   sec/tick 2141.9  sec/kimg 535.47  maintenance 5.2    cpumem 5.59   gpumem 7.05   augment 0.000\n",
            "tick 13    kimg 52.0     time 7h 47m 10s   sec/tick 2140.1  sec/kimg 535.03  maintenance 5.1    cpumem 5.78   gpumem 7.05   augment 0.000\n",
            "tick 14    kimg 56.0     time 8h 22m 55s   sec/tick 2140.2  sec/kimg 535.06  maintenance 4.8    cpumem 5.79   gpumem 7.05   augment 0.000\n",
            "tick 15    kimg 60.0     time 8h 58m 39s   sec/tick 2139.2  sec/kimg 534.79  maintenance 4.8    cpumem 5.66   gpumem 7.05   augment 0.000\n",
            "tick 16    kimg 64.0     time 9h 34m 24s   sec/tick 2139.7  sec/kimg 534.93  maintenance 5.0    cpumem 5.67   gpumem 7.05   augment 0.000\n",
            "tick 17    kimg 68.0     time 10h 10m 07s  sec/tick 2137.6  sec/kimg 534.41  maintenance 4.9    cpumem 5.74   gpumem 7.05   augment 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwPrEVh5coPf"
      },
      "source": [
        "### While it’s training...\n",
        "**Once the above cell is running you should be training!**\n",
        "\n",
        "Don’t close this tab! Colab needs to be open and running in order to continue training. \n",
        "\n",
        "Every ~40min or so a new line should get added to your output, indicated its still training. Depending on you `snapshot_count` setting you should see the results folder (`/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results`) in your Google drive folder fill with both samples (`fakesXXXXXx.jpg`) and model weights (`network-snapshot-XXXXXX.pkl`). The samples are worth looking at while it trains but don’t get too worried about each individual sample.\n",
        "\n",
        "Once Colab shuts off, you can Reconnect the notebook and re-run every cell from top to bottom. Make sure you update the `resume_from` path to continue training from the latest model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFEHPByohDdG",
        "cellView": "form"
      },
      "source": [
        "#@title Convert Legacy Model\n",
        "\n",
        "#@markdown If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this workshop you do **not** need to use this cell.\n",
        "\n",
        "#@markdown Path to model that you want to convert \n",
        "source_pkl = \"\" #@param {type: \"string\"}\n",
        "#@markdown Path and file name to convert to.\n",
        "dest_pkl = \"\" #@param {type: \"string\"}\n",
        "\n",
        "source_pkl = shlex.quote(source_pkl)\n",
        "dest_pkl = shlex.quote(dest_pkl)\n",
        "!python legacy.py --source={source_pkl} --dest={dest_pkl}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el28DuPKmQ9P"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNaj42CuwG06",
        "cellView": "form"
      },
      "source": [
        "#@title Generate Images\n",
        "#@markdown Directory to save the generated images\n",
        "outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/1\" #@param {type: \"string\"}\n",
        "#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n",
        "network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Generation Parameters**\n",
        "\n",
        "#@markdown Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n",
        "truncation = 0.8 #@param {type: \"number\"}\n",
        "#@markdown This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. Provide one number or comma-separated list of integers\n",
        "seeds = \"42,3333,80\" #@param {type: \"string\"}\n",
        "#@markdown Generate random images (`seeds` parameter will be ignored)\n",
        "gen_random = False #@param {type: \"boolean\"}\n",
        "#@markdown Amount of random images to generate\n",
        "n_imgs = 3 #@param {type: \"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown **Generate Non-Square Images**\n",
        "\n",
        "#@markdown It's possible to make the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n",
        "gen_nonsquare = False #@param {type: \"boolean\"}\n",
        "#@markdown Width\n",
        "width = 1920 #@param {type: \"integer\"}\n",
        "#@markdown Height\n",
        "height = 1080 #@param {type: \"integer\"}\n",
        "#@markdown Padding style to apply in the additional space\n",
        "scale_type = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n",
        "\n",
        "\n",
        "if gen_random:\n",
        "  seeds = ','.join(str(s) for s in list(set(list(np.random.randint(4294967295, size=n_imgs)))))\n",
        "else:\n",
        "  seeds = ','.join(str(s).strip() for s in seeds.split(','))\n",
        "print(\"Seeds: \", seeds)\n",
        "\n",
        "nonsquare = f'--size={width}-{height} --scale-type={scale_type}' if gen_nonsquare else ''\n",
        "\n",
        "\n",
        "outdir = shlex.quote(outdir)\n",
        "network = shlex.quote(network)\n",
        "!python generate.py --outdir={outdir} --trunc={truncation} --seeds={seeds} --network={network} {nonsquare}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEEr74W333U2",
        "cellView": "form"
      },
      "source": [
        "#@title Truncation Traversal\n",
        "\n",
        "#@markdown Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "#@markdown Directory to save the generated images\n",
        "outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/2\" #@param {type: \"string\"}\n",
        "#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n",
        "network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown Pass this only one seed. Pick a favorite from your generated images.\n",
        "seed = 42  #@param {type: \"integer\"}\n",
        "#@markdown Starting truncation value.\n",
        "start = -1  #@param {type: \"number\"}\n",
        "#@markdown Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "stop = 1  #@param {type: \"number\"}\n",
        "#@markdown How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n",
        "increment = 0.01  #@param {type: \"number\"}\n",
        "\n",
        "outdir = shlex.quote(outdir)\n",
        "network = shlex.quote(network)\n",
        "!python generate.py --process=\"truncation\" --outdir={outdir} --start={start} --stop={stop} --increment={increment} --seeds={seed} --network={network}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGZVg1JxlPyZ",
        "cellView": "form"
      },
      "source": [
        "#@title Interpolation\n",
        "#@markdown Directory to save the generated images\n",
        "outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/3\" #@param {type: \"string\"}\n",
        "#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n",
        "network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Generation Parameters**\n",
        "\n",
        "#@markdown Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n",
        "truncation = 0.8 #@param {type: \"number\"}\n",
        "#@markdown This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. Provide one number or comma-separated list of integers\n",
        "seeds = \"42,333\" #@param {type: \"string\"}\n",
        "#@markdown Generate random images (`seeds` parameter will be ignored)\n",
        "gen_random = False #@param {type: \"boolean\"}\n",
        "#@markdown Amount of random images to generate\n",
        "n_imgs = 10 #@param {type: \"integer\"}\n",
        "#@markdown Loop interpolation\n",
        "gen_loop = True #@param {type: \"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Interpolation Parameters**\n",
        "\n",
        "#@markdown Interpolation type\n",
        "interpolation = 'linear' #@param ['linear', 'slerp']\n",
        "#@markdown Latent space\n",
        "space = 'z' #@param ['z', 'w']\n",
        "#@markdown ---\n",
        "#@markdown **Generate Non-Square Images**\n",
        "\n",
        "#@markdown It's possible to make the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n",
        "gen_nonsquare = False #@param {type: \"boolean\"}\n",
        "#@markdown Width\n",
        "width = 1920 #@param {type: \"integer\"}\n",
        "#@markdown Height\n",
        "height = 1080 #@param {type: \"integer\"}\n",
        "#@markdown Padding style to apply in the additional space\n",
        "scale_type = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n",
        "\n",
        "\n",
        "if gen_random:\n",
        "  seeds = list(set(list(np.random.randint(4294967295, size=n_imgs))))\n",
        "  if gen_loop:\n",
        "    seeds.append(seeds[-1])\n",
        "  seeds = ','.join(str(s) for s in seeds)\n",
        "else:\n",
        "  seeds = ','.join(str(s).strip() for s in seeds.split(','))\n",
        "print(\"Seeds: \", seeds)\n",
        "\n",
        "nonsquare = f'--size={width}-{height} --scale-type={scale_type}' if gen_nonsquare else ''\n",
        "\n",
        "outdir = shlex.quote(outdir)\n",
        "network = shlex.quote(network)\n",
        "!python generate.py --outdir={outdir} --trunc={truncation} --seeds={seeds} --network={network} {nonsquare} --space={space} --process=\"interpolation\" --interpolation={interpolation}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCbgH771qK_w",
        "cellView": "form"
      },
      "source": [
        "#@title Interpolation loops\n",
        "#@markdown Directory to save the generated images\n",
        "outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/4\" #@param {type: \"string\"}\n",
        "#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n",
        "network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Generation Parameters**\n",
        "\n",
        "#@markdown Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n",
        "truncation = 0.8 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Interpolation Parameters**\n",
        "\n",
        "#@markdown Interpolation type\n",
        "interpolation = 'noiseloop' #@param ['noiseloop', 'circularloop']\n",
        "#@markdown Number of frames\n",
        "frames = 720 #@param {type: \"integer\"}\n",
        "#@markdown This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `frames` can help determine how fast the video feels.\n",
        "diameter = 0.9 #@param {type: \"number\"}\n",
        "#@markdown Starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point\n",
        "random_seed = 42\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Generate Non-Square Images**\n",
        "\n",
        "#@markdown It's possible to make the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n",
        "gen_nonsquare = True #@param {type: \"boolean\"}\n",
        "#@markdown Width\n",
        "width = 1920 #@param {type: \"integer\"}\n",
        "#@markdown Height\n",
        "height = 1080 #@param {type: \"integer\"}\n",
        "#@markdown Padding style to apply in the additional space\n",
        "scale_type = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n",
        "\n",
        "\n",
        "nonsquare = f'--size={width}-{height} --scale-type={scale_type}' if gen_nonsquare else ''\n",
        "\n",
        "outdir = shlex.quote(outdir)\n",
        "network = shlex.quote(network)\n",
        "!python generate.py --outdir={outdir} --trunc={truncation} --network={network} {nonsquare} --process=\"interpolation\" --interpolation={interpolation} --diameter={diameter} --random_seed={random_seed} --frames={frames}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}